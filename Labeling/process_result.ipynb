{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80ad7527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def mergeCsv(output_file, *input_files):\n",
    "    \"\"\"\n",
    "    Merges multiple CSV files into a single CSV file.\n",
    "\n",
    "    Parameters:\n",
    "        output_file (str): The name of the output CSV file.\n",
    "        *input_files (str): Paths to the input CSV files to be merged.\n",
    "    \"\"\"\n",
    "    # List to store DataFrames\n",
    "    dataframes = []\n",
    "\n",
    "    # Read each CSV file and append to the list\n",
    "    for file in input_files:\n",
    "        if os.path.exists(file):\n",
    "            df = pd.read_csv(file)\n",
    "            dataframes.append(df)\n",
    "        else:\n",
    "            print(f\"File not found: {file}\")\n",
    "\n",
    "    # Concatenate all DataFrames\n",
    "    if dataframes:\n",
    "        merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "        # Save the merged DataFrame to the output file\n",
    "        merged_df.to_csv(output_file, index=False)\n",
    "        print(f\"Merged CSV saved as: {output_file}\")\n",
    "    else:\n",
    "        print(\"No valid files to merge.\")\n",
    "\n",
    "\n",
    "def countRow(input_file):\n",
    "    \"\"\"\n",
    "    Counts the number of rows in a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "        input_file (str): Path to the input CSV file.\n",
    "\n",
    "    Returns:\n",
    "        int: Number of rows in the CSV file.\n",
    "    \"\"\"\n",
    "    if os.path.exists(input_file):\n",
    "        df = pd.read_csv(input_file)\n",
    "        row_count = len(df)\n",
    "        print(f\"Number of rows in {input_file}: {row_count}\")\n",
    "    else:\n",
    "        print(f\"File not found: {input_file}\")\n",
    "\n",
    "def checkDuplicate(file_path):\n",
    "    \"\"\"\n",
    "    Checks for duplicate rows in a CSV file based on the 'Domain' column.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): Path to the input CSV file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Check for duplicates based on the 'Domain' column\n",
    "        duplicates = df[df.duplicated(subset='Domain', keep=False)]\n",
    "\n",
    "        # Print the duplicates if any\n",
    "        if not duplicates.empty:\n",
    "            print(f\"Found {len(duplicates)} duplicate rows based on the 'Domain' column:\")\n",
    "        else:\n",
    "            print(\"No duplicates found based on the 'Domain' column.\")\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "\n",
    "def removeDuplicate(file_path, output_file):\n",
    "    \"\"\"\n",
    "    Removes duplicate rows in a CSV file based on the 'Domain' column, keeping the first occurrence.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): Path to the input CSV file.\n",
    "        output_file (str): Path to save the deduplicated CSV file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Remove duplicates based on the 'Domain' column, keeping the first occurrence\n",
    "        deduplicated_df = df.drop_duplicates(subset='Domain', keep='first')\n",
    "\n",
    "        # Save the deduplicated DataFrame to a new file\n",
    "        deduplicated_df.to_csv(output_file, index=False)\n",
    "        print(f\"Duplicates removed. Deduplicated file saved as: {output_file}\")\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3121e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged CSV saved as: NOSFT_netpro_raw_7k_train_label.csv\n"
     ]
    }
   ],
   "source": [
    "mergeCsv(\n",
    "    \"NOSFT_netpro_raw_7k_train_label.csv\",\n",
    "    \"NOSFT_netpro_raw_7k_train_label0-1043.csv\",\n",
    "    \"NOSFT_netpro_raw_7k_train_label1044-7244.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e62174ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged CSV saved as: ./labelling_result/2604/processed_2604/merged_fixed_2604.csv\n",
      "Number of rows in ./labelling_result/2604/processed_2604/merged_fixed_2604.csv: 2634\n"
     ]
    }
   ],
   "source": [
    "mergeCsv(\n",
    "    \"./labelling_result/2604/processed_2604/merged_fixed_2604.csv\",\n",
    "    \"./labelling_result/2604/label_2604/Fixed_-1_0-1095.csv\",\n",
    "    \"./labelling_result/2604/label_2604/Fixed_-2_0-339.csv\",\n",
    "    \"./labelling_result/2604/label_2604/Fixed_-2_340-347.csv\",\n",
    "    \"./labelling_result/2604/label_2604/Fixed_-3_0.csv\",\n",
    "    \"./labelling_result/2604/label_2604/Fixed_-4_0-431.csv\",\n",
    "    \"./labelling_result/2604/label_2604/Fixed_-5_0-78.csv\",\n",
    "    \"./labelling_result/2604/label_2604/Fixed_-6_0-135.csv\"\n",
    ")\n",
    "countRow(\"./labelling_result/2604/processed_2604/merged_fixed_2604.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d936c3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged CSV saved as: ./labelling_result/2604/processed_2604/merged_labeled_2604.csv\n",
      "Number of rows in ./labelling_result/2604/processed_2604/merged_labeled_2604.csv: 2634\n"
     ]
    }
   ],
   "source": [
    "mergeCsv(\n",
    "    \"./labelling_result/2604/processed_2604/merged_labeled_2604.csv\",\n",
    "    \"./labelling_result/2604/label_2604/Labeled_-1_0-1095.csv\",\n",
    "    \"./labelling_result/2604/label_2604/Labeled_-2_0-339.csv\",\n",
    "    \"./labelling_result/2604/label_2604/Labeled_-2_340-347.csv\",\n",
    "    \"./labelling_result/2604/label_2604/Labeled_-3_0.csv\",\n",
    "    \"./labelling_result/2604/label_2604/Labeled_-4_0-431.csv\",\n",
    "    \"./labelling_result/2604/label_2604/Labeled_-5_0-78.csv\",\n",
    "    \"./labelling_result/2604/label_2604/Labeled_-6_0-135.csv\"\n",
    ")\n",
    "countRow(\"./labelling_result/2604/processed_2604/merged_labeled_2604.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2230b609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicates found based on the 'Domain' column.\n",
      "Duplicates removed. Deduplicated file saved as: ./labelling_result/2604/processed_2604/merged_labeled_2604_dedup.csv\n",
      "Number of rows in ./labelling_result/2604/processed_2604/merged_labeled_2604_dedup.csv: 2634\n",
      "No duplicates found based on the 'Domain' column.\n"
     ]
    }
   ],
   "source": [
    "# Process `cs_54k_filtered_fixed` files\n",
    "checkDuplicate(\"./labelling_result/2604/processed_2604/merged_labeled_2604.csv\")\n",
    "removeDuplicate(\"./labelling_result/2604/processed_2604/merged_labeled_2604.csv\", \"./labelling_result/2604/processed_2604/merged_labeled_2604_dedup.csv\")\n",
    "countRow(\"./labelling_result/2604/processed_2604/merged_labeled_2604_dedup.csv\")\n",
    "checkDuplicate(\"./labelling_result/2604/processed_2604/merged_labeled_2604_dedup.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6df71fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicates found based on the 'Domain' column.\n",
      "Duplicates removed. Deduplicated file saved as: ./labelling_result/2604/processed_2604/merged_fixed_2604_dedup.csv\n",
      "Number of rows in ./labelling_result/2604/processed_2604/merged_fixed_2604_dedup.csv: 2634\n",
      "No duplicates found based on the 'Domain' column.\n"
     ]
    }
   ],
   "source": [
    "# Process `cs_54k_filtered_fixed` files\n",
    "checkDuplicate(\"./labelling_result/2604/processed_2604/merged_fixed_2604.csv\")\n",
    "removeDuplicate(\"./labelling_result/2604/processed_2604/merged_fixed_2604.csv\", \"./labelling_result/2604/processed_2604/merged_fixed_2604_dedup.csv\")\n",
    "countRow(\"./labelling_result/2604/processed_2604/merged_fixed_2604_dedup.csv\")\n",
    "checkDuplicate(\"./labelling_result/2604/processed_2604/merged_fixed_2604_dedup.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "113cb2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 duplicate rows based on the 'Domain' column:\n",
      "Duplicates removed. Deduplicated file saved as: ./labelling_result/2204/cs_54k_2204/cs_54k_filtered_fixed_39406-40930_dedup.csv\n",
      "Number of rows in ./labelling_result/2204/cs_54k_2204/cs_54k_filtered_fixed_39406-40930_dedup.csv: 1522\n",
      "No duplicates found based on the 'Domain' column.\n",
      "Found 6 duplicate rows based on the 'Domain' column:\n",
      "Duplicates removed. Deduplicated file saved as: ./labelling_result/2204/cs_54k_2204/cs_54k_filtered_label_39406-40930_dedup.csv\n",
      "Number of rows in ./labelling_result/2204/cs_54k_2204/cs_54k_filtered_label_39406-40930_dedup.csv: 1522\n",
      "No duplicates found based on the 'Domain' column.\n",
      "No duplicates found based on the 'Domain' column.\n",
      "Duplicates removed. Deduplicated file saved as: ./labelling_result/2204/cc_260k_2204/cc_260k_fixed_100000-117619_dedup.csv\n",
      "Number of rows in ./labelling_result/2204/cc_260k_2204/cc_260k_fixed_100000-117619_dedup.csv: 17620\n",
      "No duplicates found based on the 'Domain' column.\n",
      "No duplicates found based on the 'Domain' column.\n",
      "Duplicates removed. Deduplicated file saved as: ./labelling_result/2204/cc_260k_2204/cc_260k_labelled_100000-117619_dedup.csv\n",
      "Number of rows in ./labelling_result/2204/cc_260k_2204/cc_260k_labelled_100000-117619_dedup.csv: 17620\n",
      "No duplicates found based on the 'Domain' column.\n"
     ]
    }
   ],
   "source": [
    "# Process `cs_54k_filtered_fixed` files\n",
    "checkDuplicate(\"./labelling_result/2204/cs_54k_2204/cs_54k_filtered_fixed_39406-40930.csv\")\n",
    "removeDuplicate(\"./labelling_result/2204/cs_54k_2204/cs_54k_filtered_fixed_39406-40930.csv\", \"./labelling_result/2204/cs_54k_2204/cs_54k_filtered_fixed_39406-40930_dedup.csv\")\n",
    "countRow(\"./labelling_result/2204/cs_54k_2204/cs_54k_filtered_fixed_39406-40930_dedup.csv\")\n",
    "checkDuplicate(\"./labelling_result/2204/cs_54k_2204/cs_54k_filtered_fixed_39406-40930_dedup.csv\")\n",
    "\n",
    "# Process `cs_54k_filtered_label` files\n",
    "checkDuplicate(\"./labelling_result/2204/cs_54k_2204/cs_54k_filtered_label_39406-40930.csv\")\n",
    "removeDuplicate(\"./labelling_result/2204/cs_54k_2204/cs_54k_filtered_label_39406-40930.csv\", \"./labelling_result/2204/cs_54k_2204/cs_54k_filtered_label_39406-40930_dedup.csv\")\n",
    "countRow(\"./labelling_result/2204/cs_54k_2204/cs_54k_filtered_label_39406-40930_dedup.csv\")\n",
    "checkDuplicate(\"./labelling_result/2204/cs_54k_2204/cs_54k_filtered_label_39406-40930_dedup.csv\")\n",
    "\n",
    "# Process `cc_260k_fixed` files\n",
    "checkDuplicate(\"./labelling_result/2204/cc_260k_2204/cc_260k_fixed_100000-117619.csv\")\n",
    "removeDuplicate(\"./labelling_result/2204/cc_260k_2204/cc_260k_fixed_100000-117619.csv\", \"./labelling_result/2204/cc_260k_2204/cc_260k_fixed_100000-117619_dedup.csv\")\n",
    "countRow(\"./labelling_result/2204/cc_260k_2204/cc_260k_fixed_100000-117619_dedup.csv\")\n",
    "checkDuplicate(\"./labelling_result/2204/cc_260k_2204/cc_260k_fixed_100000-117619_dedup.csv\")\n",
    "\n",
    "# Process `cc_260k_labelled` files\n",
    "checkDuplicate(\"./labelling_result/2204/cc_260k_2204/cc_260k_labelled_100000-117619.csv\")\n",
    "removeDuplicate(\"./labelling_result/2204/cc_260k_2204/cc_260k_labelled_100000-117619.csv\", \"./labelling_result/2204/cc_260k_2204/cc_260k_labelled_100000-117619_dedup.csv\")\n",
    "countRow(\"./labelling_result/2204/cc_260k_2204/cc_260k_labelled_100000-117619_dedup.csv\")\n",
    "checkDuplicate(\"./labelling_result/2204/cc_260k_2204/cc_260k_labelled_100000-117619_dedup.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd6ba971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged CSV saved as: ./labelling_result/2204/processed_2204/merged_label_2204.csv\n",
      "Merged CSV saved as: ./labelling_result/2204/processed_2204/merged_fixed_2204.csv\n"
     ]
    }
   ],
   "source": [
    "mergeCsv(\n",
    "    \"./labelling_result/2204/processed_2204/merged_label_2204.csv\",\n",
    "    \"./labelling_result/2204/cc_260k_2204/cc_260k_labelled_100000-117619_dedup.csv\",\n",
    "    \"./labelling_result/2204/cs_54k_2204/cs_54k_filtered_label_39406-40930_dedup.csv\"\n",
    ")\n",
    "\n",
    "mergeCsv(\n",
    "    \"./labelling_result/2204/processed_2204/merged_fixed_2204.csv\",\n",
    "    \"./labelling_result/2204/cc_260k_2204/cc_260k_fixed_100000-117619_dedup.csv\",\n",
    "    \"./labelling_result/2204/cs_54k_2204/cs_54k_filtered_fixed_39406-40930_dedup.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4dfc8b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in ./labelling_result/2204/processed_2204/merged_label_2204.csv: 19142\n",
      "No duplicates found based on the 'Domain' column.\n",
      "Number of rows in ./labelling_result/2204/processed_2204/merged_fixed_2204.csv: 19142\n",
      "No duplicates found based on the 'Domain' column.\n"
     ]
    }
   ],
   "source": [
    "countRow(\"./labelling_result/2204/processed_2204/merged_label_2204.csv\")\n",
    "checkDuplicate(\"./labelling_result/2204/processed_2204/merged_label_2204.csv\")\n",
    "countRow(\"./labelling_result/2204/processed_2204/merged_fixed_2204.csv\")\n",
    "checkDuplicate(\"./labelling_result/2204/processed_2204/merged_fixed_2204.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2527bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Domain', 'Content', 'Label', 'Confidence']\n",
      "['Domain', 'Answer', 'Classification', 'Reason', 'Confidence', 'Thought']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_and_print_all_columns(file_path):\n",
    "    \"\"\"\n",
    "    Load a CSV file and print all columns.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): Path to the input CSV file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(df.columns.tolist())\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "load_and_print_all_columns(\"./labelling_result/2604/processed_2604/merged_fixed_2604_dedup.csv\")\n",
    "load_and_print_all_columns(\"./labelling_result/2604/processed_2604/merged_labeled_2604_dedup.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "070a3471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'Answer' has been renamed to 'Label' and saved to ./labelling_result/2204/processed_2204/merged_label_2204.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the CSV file\n",
    "file_path = \"./labelling_result/2204/processed_2204/merged_label_2204.csv\"\n",
    "if os.path.exists(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Rename the 'Answer' column to 'Label'\n",
    "    if 'Answer' in df.columns:\n",
    "        df.rename(columns={'Answer': 'Label'}, inplace=True)\n",
    "\n",
    "        # Save the updated DataFrame back to the file\n",
    "        df.to_csv(file_path, index=False)\n",
    "        print(f\"Column 'Answer' has been renamed to 'Label' and saved to {file_path}\")\n",
    "    else:\n",
    "        print(\"The 'Answer' column is not present in the file.\")\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb50ec67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged CSV saved as: ./labelling_result/2604/processed_2604/merged_combined_2604.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "label_file = \"./labelling_result/2604/processed_2604/merged_fixed_2604_dedup.csv\"\n",
    "fixed_file = \"./labelling_result/2604/processed_2604/merged_labeled_2604_dedup.csv\"\n",
    "output_file = \"./labelling_result/2604/processed_2604/merged_combined_2604.csv\"\n",
    "\n",
    "if os.path.exists(fixed_file) and os.path.exists(label_file):\n",
    "    # Load both CSV files into DataFrames\n",
    "    df_fixed = pd.read_csv(fixed_file)\n",
    "    df_label = pd.read_csv(label_file)\n",
    "\n",
    "    # Strip paragraph spacing (\\n) from the 'Thought' column in df_label\n",
    "    if 'Thought' in df_label.columns:\n",
    "        df_label['Thought'] = df_label['Thought'].str.replace('\\n', ' ', regex=False).str.strip()\n",
    "\n",
    "    # Merge the DataFrames on 'Domain', 'Label', and 'Confidence'\n",
    "    merged_df = pd.merge(df_fixed, df_label, on=['Domain'])\n",
    "    # Save the merged DataFrame to a new CSV file\n",
    "    merged_df.to_csv(output_file, index=False)\n",
    "    print(f\"Merged CSV saved as: {output_file}\")\n",
    "else:\n",
    "    print(\"One or both input files are missing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5b42b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in ./labelling_result/2604/processed_2604/merged_combined_2604.csv: 2634\n",
      "No duplicates found based on the 'Domain' column.\n",
      "['Domain', 'Answer', 'Classification', 'Reason', 'Confidence_x', 'Thought', 'Content', 'Label', 'Confidence_y']\n"
     ]
    }
   ],
   "source": [
    "countRow(\"./labelling_result/2604/processed_2604/merged_combined_2604.csv\")\n",
    "checkDuplicate(\"./labelling_result/2604/processed_2604/merged_combined_2604.csv\")\n",
    "load_and_print_all_columns(\"./labelling_result/2604/processed_2604/merged_combined_2604.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4bc2c664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows where Confidence_x does not equal Confidence_y:\n",
      "Empty DataFrame\n",
      "Columns: [Domain, Answer, Classification, Reason, Confidence_x, Thought, Content, Label, Confidence_y]\n",
      "Index: []\n",
      "Updated file saved successfully: ./labelling_result/2604/processed_2604/merged_combined_2604.csv\n"
     ]
    }
   ],
   "source": [
    "file_path = \"./labelling_result/2604/processed_2604/merged_combined_2604.csv\"\n",
    "if os.path.exists(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Filter rows where Confidence_x does not equal Confidence_y\n",
    "    mismatched_confidence = df[df['Confidence_x'] != df['Confidence_y']]\n",
    "    \n",
    "    # Print the filtered rows\n",
    "    print(\"Rows where Confidence_x does not equal Confidence_y:\")\n",
    "    print(mismatched_confidence)\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")\n",
    "\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Drop the Confidence_y column\n",
    "    if 'Confidence_y' in df.columns:\n",
    "        df.drop(columns=['Confidence_y'], inplace=True)\n",
    "    \n",
    "    # Rename Confidence_x to Confidence\n",
    "    if 'Confidence_x' in df.columns:\n",
    "        df.rename(columns={'Confidence_x': 'Confidence'}, inplace=True)\n",
    "    \n",
    "    # Save the updated DataFrame back to the file\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Updated file saved successfully: {file_path}\")\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d241c89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Domain', 'Answer', 'Classification', 'Reason', 'Confidence', 'Thought', 'Content', 'Label']\n"
     ]
    }
   ],
   "source": [
    "load_and_print_all_columns(\"./labelling_result/2604/processed_2604/merged_combined_2604.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a20cecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated file saved successfully: ./labelling_result/2604/processed_2604/merged_combined_2604.csv\n"
     ]
    }
   ],
   "source": [
    "file_path = \"./labelling_result/2604/processed_2604/merged_combined_2604.csv\"\n",
    "if os.path.exists(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Drop the 'Answer' column\n",
    "    if 'Answer' in df.columns:\n",
    "        df.drop(columns=['Answer'], inplace=True)\n",
    "    \n",
    "    # Rearrange the columns\n",
    "    column_order = ['Domain', 'Content', 'Label', 'Classification', 'Reason', 'Confidence', 'Thought']\n",
    "    df = df[column_order]\n",
    "    \n",
    "    # Save the updated DataFrame back to the file\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Updated file saved successfully: {file_path}\")\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8d8bb02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated ./labelling_result/2204/processed_2204/merged_combined_2204.csv saved successfully.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./labelling_result/2204/processed_2204/merged_combined_2204.csv\")\n",
    "\n",
    "# Drop Label_x and Confidence_x\n",
    "df.drop(columns=['Label_x', 'Confidence_x'], inplace=True)\n",
    "\n",
    "# Rename Label_y and Confidence_y to Label and Confidence\n",
    "df.rename(columns={'Label_y': 'Label', 'Confidence_y': 'Confidence'}, inplace=True)\n",
    "\n",
    "# Save the updated DataFrame back to the file\n",
    "df.to_csv(\"./labelling_result/2204/processed_2204/merged_combined_2204.csv\", index=False)\n",
    "print(f\"Updated ./labelling_result/2204/processed_2204/merged_combined_2204.csv saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1063ba57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content lengths:\n",
      "0       10008\n",
      "1       10008\n",
      "2        3569\n",
      "3        3927\n",
      "4       10008\n",
      "        ...  \n",
      "2629    10008\n",
      "2630     1889\n",
      "2631     2164\n",
      "2632     2482\n",
      "2633    10008\n",
      "Name: Content_Length, Length: 2634, dtype: int64\n",
      "\n",
      "Statistics:\n",
      "Average Content Length: 5625.039104024298\n",
      "Highest Content Length: 10008\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def analyzeContentLength(file_path):\n",
    "    \"\"\"\n",
    "    Analyzes the length of the 'Content' column in a CSV file and prints statistics.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): Path to the input CSV file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if os.path.exists(file_path):\n",
    "        # Load the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Check if 'Content' column exists\n",
    "        if 'Content' in df.columns:\n",
    "            # Calculate the length of the 'Content' column\n",
    "            df['Content_Length'] = df['Content'].apply(len)\n",
    "\n",
    "            # Print the content lengths\n",
    "            print(\"Content lengths:\")\n",
    "            print(df['Content_Length'])\n",
    "\n",
    "            # Calculate statistics\n",
    "            average_length = df['Content_Length'].mean()\n",
    "            max_length = df['Content_Length'].max()\n",
    "\n",
    "            # Print statistics\n",
    "            print(f\"\\nStatistics:\")\n",
    "            print(f\"Average Content Length: {average_length}\")\n",
    "            print(f\"Highest Content Length: {max_length}\")\n",
    "        else:\n",
    "            print(\"The 'Content' column is not present in the file.\")\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "\n",
    "# Example usage\n",
    "analyzeContentLength(\"./labelling_result/2604/processed_2604/merged_combined_2604.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c46e649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated CSV with limited content saved as: ./labelling_result/2204/processed_2204/merged_combined_limited_2204.csv\n"
     ]
    }
   ],
   "source": [
    "def limit_content_length(content: str, max_content_length: int = 10000) -> str:\n",
    "    \"\"\"\n",
    "    Limit the length of the 'Content' to a maximum length while preserving context.\n",
    "\n",
    "    Parameters:\n",
    "        content (str): The original content string.\n",
    "        max_content_length (int): The maximum allowed length for the content.\n",
    "\n",
    "    Returns:\n",
    "        str: The content trimmed to fit within the maximum length.\n",
    "    \"\"\"\n",
    "    if len(content) <= max_content_length:\n",
    "        return content\n",
    "\n",
    "    # Calculate balanced chunks\n",
    "    start_len = int(max_content_length * 0.2)  # 20% for the start\n",
    "    end_len = int(max_content_length * 0.2)    # 20% for the end\n",
    "    mid_len = max_content_length - (start_len + end_len)  # Remaining for the middle\n",
    "\n",
    "    start = content[:start_len]\n",
    "    mid_point = len(content) // 2\n",
    "    mid = content[mid_point - mid_len // 2:mid_point + mid_len // 2]\n",
    "    end = content[-end_len:]\n",
    "\n",
    "    return f\"{start}... {mid}... {end}\"\n",
    "\n",
    "def limit_content_in_csv(file_path, output_file, max_content_length=10000):\n",
    "    \"\"\"\n",
    "    Load a CSV file, limit the length of the 'Content' column to a maximum length,\n",
    "    and save the updated CSV to a new file.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): Path to the input CSV file.\n",
    "        output_file (str): Path to save the updated CSV file.\n",
    "        max_content_length (int): Maximum allowed length for the 'Content' column.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if os.path.exists(file_path):\n",
    "        # Load the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Check if 'Content' column exists\n",
    "        if 'Content' in df.columns:\n",
    "            # Apply the limit_content_length function to the 'Content' column\n",
    "            df['Content'] = df['Content'].apply(lambda x: limit_content_length(x, max_content_length))\n",
    "\n",
    "            # Save the updated DataFrame to a new file\n",
    "            df.to_csv(output_file, index=False)\n",
    "            print(f\"Updated CSV with limited content saved as: {output_file}\")\n",
    "        else:\n",
    "            print(\"The 'Content' column is not present in the file.\")\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "\n",
    "# Example usage\n",
    "limit_content_in_csv(\"./labelling_result/2204/processed_2204/merged_combined_2204.csv\", \"./labelling_result/2204/processed_2204/merged_combined_limited_2204.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e9c02180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content lengths:\n",
      "0          596\n",
      "1         1177\n",
      "2         2299\n",
      "3         7481\n",
      "4         5437\n",
      "         ...  \n",
      "19137    10008\n",
      "19138      903\n",
      "19139      341\n",
      "19140     8787\n",
      "19141     1736\n",
      "Name: Content_Length, Length: 19142, dtype: int64\n",
      "\n",
      "Statistics:\n",
      "Average Content Length: 4316.510761675896\n",
      "Highest Content Length: 10008\n"
     ]
    }
   ],
   "source": [
    "analyzeContentLength(\"./labelling_result/2204/processed_2204/merged_combined_limited_2204.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4590487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Domain</th>\n",
       "      <th>Content</th>\n",
       "      <th>Label</th>\n",
       "      <th>Classification</th>\n",
       "      <th>Reason</th>\n",
       "      <th>Confidence</th>\n",
       "      <th>Thought</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.gdql.org.cn</td>\n",
       "      <td>时代潮人 广东省归国华侨联合会 微信 | 手机版 | 无障碍 首页 省侨联概况 侨联章程 侨...</td>\n",
       "      <td>0</td>\n",
       "      <td>Benign</td>\n",
       "      <td>The domain 'gdql.org.cn' and content discuss t...</td>\n",
       "      <td>70</td>\n",
       "      <td>Okay, I need to classify the given website int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://www.gealh.com</td>\n",
       "      <td>亚洲AV无码成人网站国产动漫,亚洲AV乱码一区二区三四五六,综合偷自拍亚洲乱中文字幕,亚洲女...</td>\n",
       "      <td>2</td>\n",
       "      <td>Pornography</td>\n",
       "      <td>Content includes explicit sexual terms like '成...</td>\n",
       "      <td>40</td>\n",
       "      <td>Okay, I need to classify the website http://ww...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://www.geekiest.net</td>\n",
       "      <td>Security Geekiest.Net Technology, Smartphones,...</td>\n",
       "      <td>0</td>\n",
       "      <td>Benign</td>\n",
       "      <td>Domain 'geekiest.net' and content focus on tec...</td>\n",
       "      <td>75</td>\n",
       "      <td>Okay, I need to classify the website http://ww...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://www.georgegnall.com</td>\n",
       "      <td>9713 Inaugural Way, Montgomery Village, MD 208...</td>\n",
       "      <td>0</td>\n",
       "      <td>Benign</td>\n",
       "      <td>Domain 'georgegnall.com' suggests a real estat...</td>\n",
       "      <td>55</td>\n",
       "      <td>Okay, I need to classify the website http://ww...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.gisa.ru</td>\n",
       "      <td>Геоинформационный портал Gisa.ru - ЗАО \"Навико...</td>\n",
       "      <td>0</td>\n",
       "      <td>Benign</td>\n",
       "      <td>Domain 'gisa.ru' and content discuss GIS updat...</td>\n",
       "      <td>40</td>\n",
       "      <td>Okay, I need to classify the website http://ww...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Domain  \\\n",
       "0      http://www.gdql.org.cn   \n",
       "1        http://www.gealh.com   \n",
       "2     http://www.geekiest.net   \n",
       "3  http://www.georgegnall.com   \n",
       "4          http://www.gisa.ru   \n",
       "\n",
       "                                             Content  Label Classification  \\\n",
       "0  时代潮人 广东省归国华侨联合会 微信 | 手机版 | 无障碍 首页 省侨联概况 侨联章程 侨...      0         Benign   \n",
       "1  亚洲AV无码成人网站国产动漫,亚洲AV乱码一区二区三四五六,综合偷自拍亚洲乱中文字幕,亚洲女...      2    Pornography   \n",
       "2  Security Geekiest.Net Technology, Smartphones,...      0         Benign   \n",
       "3  9713 Inaugural Way, Montgomery Village, MD 208...      0         Benign   \n",
       "4  Геоинформационный портал Gisa.ru - ЗАО \"Навико...      0         Benign   \n",
       "\n",
       "                                              Reason  Confidence  \\\n",
       "0  The domain 'gdql.org.cn' and content discuss t...          70   \n",
       "1  Content includes explicit sexual terms like '成...          40   \n",
       "2  Domain 'geekiest.net' and content focus on tec...          75   \n",
       "3  Domain 'georgegnall.com' suggests a real estat...          55   \n",
       "4  Domain 'gisa.ru' and content discuss GIS updat...          40   \n",
       "\n",
       "                                             Thought  \n",
       "0  Okay, I need to classify the given website int...  \n",
       "1  Okay, I need to classify the website http://ww...  \n",
       "2  Okay, I need to classify the website http://ww...  \n",
       "3  Okay, I need to classify the website http://ww...  \n",
       "4  Okay, I need to classify the website http://ww...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./labelling_result/2204/processed_2204/merged_combined_limited_2204.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a60aeca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>19142.000000</td>\n",
       "      <td>19142.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.096698</td>\n",
       "      <td>81.478947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.455077</td>\n",
       "      <td>21.228752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>70.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>90.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Label    Confidence\n",
       "count  19142.000000  19142.000000\n",
       "mean       0.096698     81.478947\n",
       "std        0.455077     21.228752\n",
       "min       -1.000000      0.000000\n",
       "25%        0.000000     70.000000\n",
       "50%        0.000000     90.000000\n",
       "75%        0.000000    100.000000\n",
       "max        3.000000    100.000000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2a832d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Occurrences of each value in the 'Label' column:\n",
      "Label\n",
      "-1      177\n",
      " 0    17778\n",
      " 1      474\n",
      " 2      585\n",
      " 3      128\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "label_stats = df['Label'].value_counts().sort_index()\n",
    "\n",
    "# Print the statistics\n",
    "print(\"Occurrences of each value in the 'Label' column:\")\n",
    "print(label_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4bad3808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Domain, Content, Label, Classification, Reason, Confidence, Thought]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "print(df[df['Confidence'] == 110])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8cdd09b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "label3df = df[df['Label'] == 3]\n",
    "label3df.to_csv(\"merged_fixed_limited_label3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "08faf496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                Domain  \\\n",
      "1011                4frontierscorp.com   \n",
      "4682                        001xxx.com   \n",
      "5751                 bloggerjateng.com   \n",
      "7152                 prediksitogel.net   \n",
      "10310       jilbaberketat.blogspot.com   \n",
      "...                                ...   \n",
      "42822  https://waynes-color-centre.com   \n",
      "53494           http://powergamer.info   \n",
      "55436       https://www.deksomboon.com   \n",
      "57305         https://iki-ichifuji.com   \n",
      "70850           https://search.arch.be   \n",
      "\n",
      "                                                 Content  Label  Confidence  \n",
      "1011   4 Frontiers Corp HOME Educators Company Media ...      0          80  \n",
      "4682   Skip to content My Blog Sample Page Homeformat...      0          90  \n",
      "5751   Skip to content BLOGGER JATENG tempat kumpul b...      0          80  \n",
      "7152   Excellent 4.6 out of 5 Trustpilot The domain n...      0          65  \n",
      "10310  TitleBlog Kumpulan Video Jilboobs EXCLUSIVE 20...      0          60  \n",
      "...                                                  ...    ...         ...  \n",
      "42822  Panduan Lengkap Bermain Slot Demo: Cara Menikm...      0          65  \n",
      "53494  Roleplaying – power gamer Skip to content Augu...      0          80  \n",
      "55436  กุ้งอบวุ้นเส้น i-chef อบหม้อดิน TH EN/CH บริษั...      0          60  \n",
      "57305  ブログサンプル5 | 壱岐　ゲストハウス　一富士 BLOG ホーム ブログ 店舗写真 御膳料...      0          30  \n",
      "70850  Akte HUBRA_00225532_0 NL FR DE EN Een vraag of...      0          70  \n",
      "\n",
      "[97 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "filtered_data = df[(df['Content'].str.contains(\"togel\", case=False, na=False)) & (df['Label'].isin([0]))]\n",
    "\n",
    "# Print the filtered rows\n",
    "print(filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "efa929fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data.to_csv(\"merged_fixed_limited_togel.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad3911c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in ./labelling_result/2604/processed_2604/merge_combined_93k.csv: 93096\n",
      "Found 2946 duplicate rows based on the 'Domain' column:\n",
      "['Domain', 'Content', 'Label', 'Classification', 'Reason', 'Confidence', 'Thought']\n"
     ]
    }
   ],
   "source": [
    "countRow(\"./labelling_result/2604/processed_2604/merge_combined_93k.csv\")\n",
    "checkDuplicate(\"./labelling_result/2604/processed_2604/merge_combined_93k.csv\")\n",
    "load_and_print_all_columns(\"./labelling_result/2604/processed_2604/merge_combined_93k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "431d1462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate rows found based on the 'Domain' column:\n",
      "                                                  Domain  \\\n",
      "364                                       xhamster7.desi   \n",
      "483               www.redhat.com/advice/ask_pbrown1.html   \n",
      "582                                         bucetas.blog   \n",
      "903                                 apartamentoclick.com   \n",
      "1614   www.wired.com/techbiz/it/magazine/16-10/mf_chrome   \n",
      "...                                                  ...   \n",
      "92705                       www.patronsoft.com/firstspot   \n",
      "92706              angelsan.free.fr/blog.php?page_id=542   \n",
      "92707                             pinardslandscaping.com   \n",
      "92709                          kcs365online.blogspot.com   \n",
      "93029                                     mainskor88.lol   \n",
      "\n",
      "                                                 Content  Label  \\\n",
      "364    Gay • SG Živý Sex Prémiová Videa Pornohvězdy K...      2   \n",
      "483    user account menu log in new developments at o...      0   \n",
      "582    Bucetas.blog OS MELHORES VIDEOS PORNÔ GRATIS B...      2   \n",
      "903    Registrati Accedi MONOPOLY Casino Bienvenido a...      1   \n",
      "1614   skip to main content open navigation menu secu...      0   \n",
      "...                                                  ...    ...   \n",
      "92705  home blog & news firstspot® easegate™ channel ...      0   \n",
      "92706  portail freebox bas débit webmail mon compte p...      0   \n",
      "92707  DAFTAR LOGIN BANK pemuda_togel +6282140777941 ...      1   \n",
      "92709  KCS365 Agen Judi Online Terpercaya di Indonesi...      1   \n",
      "93029  {{-- --}} Download APK | Live Chat | Hubungi K...      1   \n",
      "\n",
      "      Classification                                             Reason  \\\n",
      "364      Pornography  Domain 'xhamster7.desi' and content include ex...   \n",
      "483           Benign  The domain 'www.redhat. com' and content discu...   \n",
      "582      Pornography  Domain 'bucetas.blog' and content include expl...   \n",
      "903         Gambling  Content explicitly mentions 'Monopoly Casino' ...   \n",
      "1614          Benign  Domain 'www.wired.com' and content discuss tec...   \n",
      "...              ...                                                ...   \n",
      "92705         Benign  Domain 'patronsoft.com' and content discuss ho...   \n",
      "92706         Benign  The domain and content discuss general interne...   \n",
      "92707       Gambling  Domain 'pinardslandscaping.com' seems unrelate...   \n",
      "92709       Gambling  Domain 'kcs365online.blogspot.com' and content...   \n",
      "93029       Gambling  The domain 'mainskor88. lol' and content inclu...   \n",
      "\n",
      "       Confidence                                            Thought  \n",
      "364            95  Okay, I need to classify this website into one...  \n",
      "483            70  Okay, I need to classify the given website int...  \n",
      "582           100  Okay, I need to classify this website into one...  \n",
      "903            70  Okay, I need to classify the website \"apartame...  \n",
      "1614           95  Okay, I need to classify the given website int...  \n",
      "...           ...                                                ...  \n",
      "92705         100  Okay, I need to classify this website into one...  \n",
      "92706          60  Okay, I need to classify this website into one...  \n",
      "92707          70  Okay, I need to classify this website into one...  \n",
      "92709          85  Okay, I need to classify this website into one...  \n",
      "93029          85  Okay, I need to classify the website mainskor8...  \n",
      "\n",
      "[2946 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = \"./labelling_result/2604/processed_2604/merge_combined_93k.csv\"\n",
    "if os.path.exists(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Find duplicate rows based on the 'Domain' column\n",
    "    duplicates = df[df.duplicated(subset='Domain', keep=False)]\n",
    "    \n",
    "    # Print the duplicate rows\n",
    "    if not duplicates.empty:\n",
    "        print(\"Duplicate rows found based on the 'Domain' column:\")\n",
    "        print(duplicates)\n",
    "    else:\n",
    "        print(\"No duplicate rows found based on the 'Domain' column.\")\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aeb223c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate rows found based on the 'Domain' column:\n",
      "Domains with mismatched labels:\n",
      "                               Domain  \\\n",
      "94026                      1stsex.com   \n",
      "58269                      1stsex.com   \n",
      "83565                    303cash.link   \n",
      "58272                    303cash.link   \n",
      "93039              abc-du-gratuit.net   \n",
      "...                               ...   \n",
      "58328  www.i-modernist.com/emulaxian/   \n",
      "57604       www.simdesign.nl/fft.html   \n",
      "93472       www.simdesign.nl/fft.html   \n",
      "21810  www.smittyware.com/palm/upirc/   \n",
      "58249  www.smittyware.com/palm/upirc/   \n",
      "\n",
      "                                                 Content  Label  \\\n",
      "94026  Pusat Layanan Klien Bantuan Masuk Lanjut Mulai...      2   \n",
      "58269  Pusat Layanan Klien Bantuan Masuk Lanjut Mulai...      0   \n",
      "83565  303CASH SITUS JUDI ONLINE UANG ASLI TERPERCAYA...      1   \n",
      "58272  303CASH SITUS JUDI ONLINE UANG ASLI TERPERCAYA...      3   \n",
      "93039  L'ABC du Gratuit...Pour trouver les meilleurs ...      0   \n",
      "...                                                  ...    ...   \n",
      "58328  aller au contenu imodernist le futur est déjà ...      1   \n",
      "57604  ga naar de inhoud sim design voorbeeld pagina ...      0   \n",
      "93472  ga naar de inhoud sim design voorbeeld pagina ...      3   \n",
      "21810  skip to content smittyware home about contact ...      1   \n",
      "58249  skip to content smittyware home about contact ...      0   \n",
      "\n",
      "      Classification                                             Reason  \\\n",
      "94026    Pornography  Domain '1stsex.com' contains explicit keywords...   \n",
      "58269         Benign  Domain '1stsex.com' and content list various d...   \n",
      "83565       Gambling  Domain '303cash.ink' and content include keywo...   \n",
      "58272        Harmful  The domain '303cash.link' and content explicit...   \n",
      "93039         Benign  The domain 'abc-du-gratuit. net' and content d...   \n",
      "...              ...                                                ...   \n",
      "58328       Gambling  The content discusses various online casinos, ...   \n",
      "57604         Benign  The domain 'simdesign.nl' and the content disc...   \n",
      "93472        Harmful  The content includes promotional information a...   \n",
      "21810       Gambling  Content discusses '먹튀검증 서비스' (scam verificatio...   \n",
      "58249         Benign  The domain and content include mentions of spo...   \n",
      "\n",
      "       Confidence                                            Thought  \n",
      "94026          80  Okay, so I need to classify this website into ...  \n",
      "58269          90  Okay, so I need to classify this website into ...  \n",
      "83565          95  Okay, I need to classify the website \"303cash....  \n",
      "58272          95  Okay, I need to classify this website into one...  \n",
      "93039          70  Okay, I need to classify this website into one...  \n",
      "...           ...                                                ...  \n",
      "58328          70  Okay, I need to classify the given website int...  \n",
      "57604         100  Alright, let's tackle this classification step...  \n",
      "93472          70  Alright, I'm looking at this website classific...  \n",
      "21810          60  Okay, I need to classify the given website int...  \n",
      "58249          50  Okay, I need to classify the given website int...  \n",
      "\n",
      "[100 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = \"./labelling_result/2604/processed_2604/merged_combined_dedup_final.csv\"\n",
    "if os.path.exists(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Find duplicate rows based on the 'Domain' column\n",
    "    duplicates = df[df.duplicated(subset='Domain', keep=False)]\n",
    "    \n",
    "    # Check if there are duplicates\n",
    "    if not duplicates.empty:\n",
    "        print(\"Duplicate rows found based on the 'Domain' column:\")\n",
    "        \n",
    "        # Sort duplicates by 'Domain' to ensure proper grouping\n",
    "        duplicates = duplicates.sort_values(by='Domain')\n",
    "        \n",
    "        # Group by 'Domain' and filter where the 'Label' values are not the same\n",
    "        mismatched_labels = duplicates.groupby('Domain').filter(\n",
    "            lambda group: group['Label'].nunique() > 1\n",
    "        )\n",
    "        \n",
    "        # Print the mismatched domains\n",
    "        if not mismatched_labels.empty:\n",
    "            print(\"Domains with mismatched labels:\")\n",
    "            print(mismatched_labels)\n",
    "        else:\n",
    "            print(\"No domains found where the labels differ between occurrences.\")\n",
    "    else:\n",
    "        print(\"No duplicate rows found based on the 'Domain' column.\")\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0774c4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First occurrences saved to: ./labelling_result/2604/processed_2604/duplicated3.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# File paths\n",
    "input_file = \"./labelling_result/2604/processed_2604/merged_combined_dedup_final.csv\"\n",
    "output_file = \"./labelling_result/2604/processed_2604/duplicated3.csv\"\n",
    "\n",
    "# Check if the input file exists\n",
    "if os.path.exists(input_file):\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(input_file)\n",
    "    \n",
    "    # Drop duplicates based on the 'Domain' column, keeping the first occurrence\n",
    "    deduplicated_df = df.drop_duplicates(subset='Domain', keep='first')\n",
    "    \n",
    "    # Save the deduplicated DataFrame to a new file\n",
    "    deduplicated_df.to_csv(output_file, index=False)\n",
    "    print(f\"First occurrences saved to: {output_file}\")\n",
    "else:\n",
    "    print(f\"File not found: {input_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540df28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in ./labelling_result/2604/processed_2604/duplicated3.csv: 93953\n",
      "No duplicates found based on the 'Domain' column.\n",
      "['Domain', 'Content', 'Label', 'Classification', 'Reason', 'Confidence', 'Thought']\n"
     ]
    }
   ],
   "source": [
    "countRow(\"./labelling_result/2604/processed_2604/duplicated3.csv\")\n",
    "checkDuplicate(\"./labelling_result/2604/processed_2604/duplicated3.csv\")\n",
    "load_and_print_all_columns(\"./labelling_result/2604/processed_2604/duplicated3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2194887c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in ./labelling_result/2604/processed_2604/merged_combined_dedup_final.csv: 96926\n",
      "Found 4491 duplicate rows based on the 'Domain' column:\n",
      "['Domain', 'Content', 'Label', 'Classification', 'Reason', 'Confidence', 'Thought']\n"
     ]
    }
   ],
   "source": [
    "countRow(\"./labelling_result/2604/processed_2604/merged_combined_dedup_final.csv\")\n",
    "checkDuplicate(\"./labelling_result/2604/processed_2604/merged_combined_dedup_final.csv\")\n",
    "load_and_print_all_columns(\"./labelling_result/2604/processed_2604/merged_combined_dedup_final.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
